{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOyizHvV99k/rC7pTHUAl0N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wuY6dYH4w_yf","executionInfo":{"status":"ok","timestamp":1730298373461,"user_tz":-330,"elapsed":440,"user":{"displayName":"Harsh Jain","userId":"05838684457085731246"}},"outputId":"b5514754-6586-49eb-f7b3-180a585132c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["City    Chicago  Los Angeles  New York  All\n","Gender                                     \n","Female        1            1         1    3\n","Male          1            1         1    3\n","All           2            2         2    6\n"]},{"output_type":"execute_result","data":{"text/plain":["{'Gender': ['Male', 'Female', 'Female', 'Male', 'Female', 'Male'],\n"," 'City': ['New York',\n","  'Los Angeles',\n","  'New York',\n","  'Chicago',\n","  'Chicago',\n","  'Los Angeles']}"]},"metadata":{},"execution_count":3}],"source":["import pandas as pd\n","\n","# Sample data\n","data = {\n","    'Gender': ['Male',   'Female',       'Female',   'Male',   'Female',    ' Male'],\n","    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Chicago', 'Los Angeles']\n","}\n","\n","# Creating DataFrame\n","df = pd.DataFrame(data)\n","\n","# Using pd.crosstab to get frequency count of each gender in each city\n","result = pd.crosstab(df['Gender'], df['City'], margins=True)\n","\n","print(result)\n","data"]},{"cell_type":"code","source":["from sklearn.datasets import make_classification\n","from collections import Counter\n","\n","# Generate a simple dataset for demonstration\n","# Setting n_informative=2 and n_redundant=0 to match n_features=2\n","X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_classes=2, weights=[0.7, 0.3], random_state=42)\n","\n","# Calculate the majority class\n","most_common_class = Counter(y).most_common(1)[0][0]\n","\n","# Predict the majority class for all instances\n","y_pred = [most_common_class] * len(y)\n","\n","# Calculate accuracy of Naive Rule\n","accuracy = sum(y_pred == y) / len(y)\n","print(f\"Naive Rule Accuracy: {accuracy:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wxVtaVp10b_B","executionInfo":{"status":"ok","timestamp":1730300217076,"user_tz":-330,"elapsed":507,"user":{"displayName":"Harsh Jain","userId":"05838684457085731246"}},"outputId":"5f745fdc-61e1-451c-b6d1-0715f6e209b1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Naive Rule Accuracy: 0.70\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","# Load the dataset\n","file_path = 'wdbc.csv'\n","df = pd.read_csv(file_path, header=None)\n","\n","# Renaming columns for clarity\n","column_names = ['ID', 'Diagnosis'] + [f'Feature_{i}' for i in range(1, 31)]\n","df.columns = column_names\n","\n","# Drop the ID column and encode 'Diagnosis'\n","df = df.drop(columns=['ID'])\n","df['Diagnosis'] = df['Diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n","\n","# Separate features and target variable\n","X = df.drop(columns=['Diagnosis'])\n","y = df['Diagnosis']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Initialize and train the Linear Regression model\n","lr_model = LinearRegression()\n","lr_model.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = lr_model.predict(X_test)\n","\n","# Calculate performance metrics\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Print the results\n","print(f\"Mean Squared Error (MSE): {mse}\")\n","print(f\"R-squared (R2 Score): {r2}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2gmcBTo9In2","executionInfo":{"status":"ok","timestamp":1730301082246,"user_tz":-330,"elapsed":449,"user":{"displayName":"Harsh Jain","userId":"05838684457085731246"}},"outputId":"56bc39f3-2841-43c6-8786-0d185902a9be"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error (MSE): 0.06728376859363186\n","R-squared (R2 Score): 0.7108399944964154\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.naive_bayes import GaussianNB\n","\n","# Load the dataset\n","file_path = 'wdbc.csv'\n","df = pd.read_csv(file_path, header=None)\n","\n","# Renaming columns for clarity\n","column_names = ['ID', 'Diagnosis'] + [f'Feature_{i}' for i in range(1, 31)]\n","df.columns = column_names\n","\n","# Drop the ID column and encode 'Diagnosis'\n","df = df.drop(columns=['ID'])\n","df['Diagnosis'] = df['Diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n","\n","# Separate features and target variable\n","X = df.drop(columns=['Diagnosis'])\n","y = df['Diagnosis']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define the baseline model (mean of the target variable)\n","baseline_prediction = y_test.mean()\n","baseline_mse = mean_squared_error(y_test, [baseline_prediction] * len(y_test))\n","\n","# Initialize and train the Linear Regression model\n","lr_model = LinearRegression()\n","lr_model.fit(X_train, y_train)\n","\n","# Make predictions on the test data for Linear Regression\n","y_pred_lr = lr_model.predict(X_test)\n","\n","# Calculate performance metrics for Linear Regression\n","mse_lr = mean_squared_error(y_test, y_pred_lr)\n","r2_lr = r2_score(y_test, y_pred_lr)\n","\n","# Initialize and train the Naive Bayes model\n","nb_model = GaussianNB()\n","nb_model.fit(X_train, y_train)\n","\n","# Make predictions on the test data for Naive Bayes\n","y_pred_nb = nb_model.predict(X_test)\n","\n","# Calculate performance metrics for Naive Bayes\n","mse_nb = mean_squared_error(y_test, y_pred_nb)\n","r2_nb = r2_score(y_test, y_pred_nb)\n","\n","# Print the results\n","print(f\"Baseline Model Mean Squared Error (MSE): {baseline_mse:.4f}\")\n","print(f\"Linear Regression Model Mean Squared Error (MSE): {mse_lr:.4f}, R-squared: {r2_lr:.4f}\")\n","print(f\"Naive Bayes Model Mean Squared Error (MSE): {mse_nb:.4f}, R-squared: {r2_nb:.4f}\")\n","\n","# Check if the linear regression model outperforms the baseline\n","if mse_lr < baseline_mse:\n","    print(\"The Linear Regression model has a better score than the baseline model.\")\n","else:\n","    print(\"The Linear Regression model does not have a better score than the baseline model.\")\n","\n","# Check if the Naive Bayes model outperforms the baseline\n","if mse_nb < baseline_mse:\n","    print(\"The Naive Bayes model has a better score than the baseline model.\")\n","else:\n","    print(\"The Naive Bayes model does not have a better score than the baseline model.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qR5x4BdT-yq2","executionInfo":{"status":"ok","timestamp":1730301613186,"user_tz":-330,"elapsed":464,"user":{"displayName":"Harsh Jain","userId":"05838684457085731246"}},"outputId":"5bf066f4-1a08-4a2d-e250-22a2beba39e6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline Model Mean Squared Error (MSE): 0.2327\n","Linear Regression Model Mean Squared Error (MSE): 0.0673, R-squared: 0.7108\n","Naive Bayes Model Mean Squared Error (MSE): 0.0585, R-squared: 0.7487\n","The Linear Regression model has a better score than the baseline model.\n","The Naive Bayes model has a better score than the baseline model.\n"]}]},{"cell_type":"code","source":["pip install ucimlrepo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LxRQKF4YCOJL","executionInfo":{"status":"ok","timestamp":1730302028951,"user_tz":-330,"elapsed":12057,"user":{"displayName":"Harsh Jain","userId":"05838684457085731246"}},"outputId":"99ea518e-8282-4687-e4c1-85661d444df5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ucimlrepo\n","  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n","Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.2.2)\n","Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.8.30)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n","Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n","Installing collected packages: ucimlrepo\n","Successfully installed ucimlrepo-0.0.7\n"]}]},{"cell_type":"code","source":["from ucimlrepo import fetch_ucirepo\n","\n","# fetch dataset\n","iris = fetch_ucirepo(id=53)\n","\n","# data (as pandas dataframes)\n","X = iris.data.features\n","y = iris.data.targets\n","\n","# metadata\n","print(iris.metadata)\n","\n","# variable information\n","print(iris.variables)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HvJRuU4zCXrX","executionInfo":{"status":"ok","timestamp":1730302031251,"user_tz":-330,"elapsed":476,"user":{"displayName":"Harsh Jain","userId":"05838684457085731246"}},"outputId":"5a3de77e-dda2-45b3-8874-58a625ee3238"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["{'uci_id': 53, 'name': 'Iris', 'repository_url': 'https://archive.ics.uci.edu/dataset/53/iris', 'data_url': 'https://archive.ics.uci.edu/static/public/53/data.csv', 'abstract': 'A small classic dataset from Fisher, 1936. One of the earliest known datasets used for evaluating classification methods.\\n', 'area': 'Biology', 'tasks': ['Classification'], 'characteristics': ['Tabular'], 'num_instances': 150, 'num_features': 4, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1936, 'last_updated': 'Tue Sep 12 2023', 'dataset_doi': '10.24432/C56C76', 'creators': ['R. A. Fisher'], 'intro_paper': {'ID': 191, 'type': 'NATIVE', 'title': 'The Iris data set: In search of the source of virginica', 'authors': 'A. Unwin, K. Kleinman', 'venue': 'Significance, 2021', 'year': 2021, 'journal': 'Significance, 2021', 'DOI': '1740-9713.01589', 'URL': 'https://www.semanticscholar.org/paper/4599862ea877863669a6a8e63a3c707a787d5d7e', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'This is one of the earliest datasets used in the literature on classification methods and widely used in statistics and machine learning.  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is linearly separable from the other 2; the latter are not linearly separable from each other.\\n\\nPredicted attribute: class of iris plant.\\n\\nThis is an exceedingly simple domain.\\n\\nThis data differs from the data presented in Fishers article (identified by Steve Chadwick,  spchadwick@espeedaz.net ).  The 35th sample should be: 4.9,3.1,1.5,0.2,\"Iris-setosa\" where the error is in the fourth feature. The 38th sample: 4.9,3.6,1.4,0.1,\"Iris-setosa\" where the errors are in the second and third features.  ', 'purpose': 'N/A', 'funded_by': None, 'instances_represent': 'Each instance is a plant', 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': None, 'citation': None}}\n","           name     role         type demographic  \\\n","0  sepal length  Feature   Continuous        None   \n","1   sepal width  Feature   Continuous        None   \n","2  petal length  Feature   Continuous        None   \n","3   petal width  Feature   Continuous        None   \n","4         class   Target  Categorical        None   \n","\n","                                         description units missing_values  \n","0                                               None    cm             no  \n","1                                               None    cm             no  \n","2                                               None    cm             no  \n","3                                               None    cm             no  \n","4  class of iris plant: Iris Setosa, Iris Versico...  None             no  \n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define and train the baseline model (Logistic Regression)\n","baseline_model = LogisticRegression(max_iter=200)\n","baseline_model.fit(X_train, y_train)\n","baseline_predictions = baseline_model.predict(X_test)\n","baseline_accuracy = accuracy_score(y_test, baseline_predictions)\n","\n","# Define and train a more complex model (Random Forest Classifier)\n","ml_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","ml_model.fit(X_train, y_train)\n","ml_predictions = ml_model.predict(X_test)\n","ml_accuracy = accuracy_score(y_test, ml_predictions)\n","\n","# Print the results\n","print(f\"Baseline Model Accuracy: {baseline_accuracy:.2f}\")\n","print(f\"ML Model Accuracy: {ml_accuracy:.2f}\")\n","\n","# Assert that the ML model should have a better accuracy than the baseline\n","ml_accuracy > baseline_accuracy, \"ML model should have a better score than the baseline model\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-gl7vwOQ7gIC","executionInfo":{"status":"ok","timestamp":1730302051904,"user_tz":-330,"elapsed":1145,"user":{"displayName":"Harsh Jain","userId":"05838684457085731246"}},"outputId":"8f49f274-a7c1-42cb-c918-2b676aaeedb5"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Baseline Model Accuracy: 1.00\n","ML Model Accuracy: 1.00\n"]},{"output_type":"execute_result","data":{"text/plain":["(False, 'ML model should have a better score than the baseline model')"]},"metadata":{},"execution_count":14}]}]}